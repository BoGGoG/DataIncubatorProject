This project is about analyzing data from job postings on Linkedin (data from Thinkium) combined with various other sources of data. In my draft I focus on a specific company, but the full project should give a way to automate this for any company. I focus on the company "Activision". Further data sources are: Google Trends for "Activision", stocks data from Yahoo finance and survey data from Glassdoor.

The idea is to find correlations between the data and a way to predict one (or all) from the combined past of all of them. For example one could ask if employee satisfaction can predict job openings or if employee satisfaction and job openings together can predict the stock price on a medium time scale. The goal is not to predict stock price on a short time scale. Also it might be of great interest for a company how job openings on Linkedin can influence their stock price. Is it better to post the job opening now or tomorrow? When in the next week would be the ideal time?

The project plan is the following:
- For one company (Activision) collect as much data as possible and combine it into one file. From the Linkedin data I one can get the following features of the time series: number of employees, followers, open positions (in a specific time frame), the google trend, stock price, difference between high and low of the stock (day), trade volume (day), employees in the same industry as the company, followers of companies in the same industry, job openings in the same industry, employee satisfaction (Glassdoor). Note: I have not yet been able to download the Glassdoor data, so it is not in the notebook yet.
- Use different time series prediction methods to predict the general trend. First try simple ones like Facebook Prophet, and simple Neural Networks. Then try different standard multivariate time series prediction methods. Also a flag for product releases or shitstorms would be a good feature for those models. The ultimate goal would be a hierarchical Bayesian model.
- Generalize and automate this workflow for other companies.
- Use data from all companies to build a hierarchical model. Basically the idea is that also in the data of the other companies there is information that can help predicting our chosen company.

I have set up two jupyter notebooks as a first draft. The first one collects the data, resamples and saves into a .csv. Note that here for simplicity I resampled on a monthly basis. In practice one would have to look at different sampling times. There is for example no way of predicting the perfect day to post a job offer, when the data is only monthly. The second notebook shows some basic plots in order to see some correlations.

The first of the two nontrivial plots is the time series of 10 different features for one specific company, in this case Activision. There is a lot of information in this plot and we can already draw some first conclusions. For example we can see that the google trend, the trade volume and the daily difference between stock high and low are related. A first simple Principal Component Analysis (PCA) shows that in good approximation 3 of the 10 features do linearly depend on the others and the rest is also not fully independent. There of course should be nonlinear correlations that one does not see by eye or PCA. A full analysis should also reveal correlations between one variable and one (or more) variables at later times. This is called "lag variables" and shown in the second plot. Basically the second plot shows some structure and thus tells us that the variables are not totally random. 

In conclusion I propose a project for analyzing the time series data from different sources for different companies, first separately and then combined into a hierarchical model, to be able to predict either the general trend of its stock price, the general health of the company or the ideal time (in the range of days) for the company to post new job offerings based on past influences on the stock price.
